---
title: "资源系统-服务端"
date: 2025-07-09
author: cimaStone
category: "技术架构/资源服务/服务端"
tags: 
  - 技术栈
  - zookeeper
  - redis
---

# 资源系统-服务端
## 🎯 背景与目标
**背景**：业务系统获取业务信息需从第三方平台获取，第三方平台以账户形式进行售卖，业务人员通过账户进行密码登录，输入行业业务指令获取资源数据，也可以通过系统交互的方式进行登录发送指令请求；账户资源有以下要求：
1. 每个账户的发起指令数量有限（数量/月），超出月租数量以阶梯式收费
2. 每个账户同一时间只能在一个终端进行登录，同时在多个终端登录会被中断连接，频繁会使账户资源冻结
3. 业务系统/资源系统都为集群部署，现业务采购1000个账户，有不同的部门不同的模块对应的业务系统都需要使用该批资源

**目标**：
  1. 将1000个账户根据业务模块及各业务所需指令数进行合理分配账户，承接业务需求
  2. 实时监控每个账户指令使用情况，动态调整每个模块&系统所分配的账户资源，最大利用账户资源，避免增加额外费用

## 统一语言
| 名称 | 含义 |
| :---: | :---: |
| pid | 第三方平台的单个账户|
| 资源系统 | 管理、分配和动态调整pid的系统 |
| 客户端 | 需要使用pid的业务系统 |
| 客户端服务器 | 需要使用pid的业务系统#某台服务器 |
| 模块 | 需要使用pid的业务系统所属模块 |
| 通知节点 | 资源系统follow服务器收到pid调度请求时，follow不能操作节点，只能把该请求插入操作记录表，并改动zk通知节点data数据为待处理（DEALING） |

## 🔧 核心流程与控制点
### 资源系统集群服务启动leader服务器启动流程（首次获取leader｜follow -> leader）
1. 需要只有一台服务器的对资源进行分配操作 && 操作zk节点通知客户端，采用zk 选举leader,非leader服务器接收业务操作人员请求后，会将请求内容转换为操作记录表
2. leader服务器被通知成为leader后：
   - 将通知节点对应的data数据改为已处理（DEALED），操作记录表数据改为已处理
   - 设置node属性isLeader == true
   - 清理数据库映射的内存数据（PidCollection#pidWithModule） && zk 映射的内存数据（PidCollection#pidMap）
   - 读取数据库数据初始化内存数据（PidCollection#pidWithModule）
   - 根据模块系统预先设定的服务器数量将分配给该模块的pid集合进行预分批
   - 检查模块路径是否在zk生成节点，已生成则监听，未生成，创建并监听
         
### 客户端服务器启动与资源系统交互流程（客户端zk节点未创建）
1. 资源系统leader服务器启动时监听模块节点
2. 客户端服务器启动时会监听所配置的模块节点，并在模块节点下创建临时节点
3. 资源系统leader服务器监听到临时节点后
   - 生成持久化节点的zk路径（临时节点路径 + 固定后缀）
   - 根据持久化节点路径创建zk节点
   - 修改临时节点data值为持久化节点路径
4. 客户端服务器监听到临时节点数据有变化，获取data数据，并监听持久化节点
5. 资源系统leader服务器监听到创建的持久化节点后，，从数据库映射的内存数据（PidCollection#pidWithModule）选取一份`List<pidModle>`批量放入该持久化节点的子路径中，并监听每个pid zk节点
  
### 客户端服务器启动与资源系统交互流程（客户端zk节点已创建）
1. 资源系统leader服务器启动时监听模块节点
2. 监听到所有模块下的临时节点和持久化节点事件，只处理持久化节点新增事件
3. 将持久化节点（模块）下的List```<Pid>```与数据库映射的内存数据（PidCollection#pidWithModule）进行比较，以数据库映射到的内存数据为准；**先操作服务器zk pid删除，再统一操作新增**，此处使用了CyclicBarrier
4. 当zk节点的数据与数据库映射的内存数据一致时，将数据写入zk 映射的内存数据（PidCollection#pidMap）

### 业务操作人员操作pid同步更新至客户端流程 - leader/follow/客户端处理流程
1. 业务操作人员在页面端操作pid
   - 针对某个模块批量新增/更新/删除pid
   - 将某些pid从A模块下架，在B模块上架
   - 重置pid的指令数量等
2. 请求至资源系统
   - 如果该服务器为leader
     1. 将需要操作的List`<pid>`通过module进行分类，抽出需要删除、新增和更新的列表
     2. 先处理删除数据，更新数据库映射的内存数据（PidCollection#pidWithModule）
     3. 更新zk节点，操作节点成功后更新zk映射的内存数据（PidCollection#pidMap）
     4. 处理新增数据，根据客户端服务器数量进行预分配，更新数据库映射的内存数据（PidCollection#pidWithModule）
     5. 更新zk节点，操作节点成功后更新zk映射的内存数据（PidCollection#pidMap）
     6. 更新List`<pid>`时，则更新数据库映射的内存数据（PidCollection#pidWithModule），更新zk节点，操作节点成功后更新zk映射的内存数据（PidCollection#pidMap）
     7. 全部处理完成后更新数据库数据
   - 如果服务器为follow
      1. 将请求数据转为操作记录表
         - 批量新增/更新/删除：生成同一个批次号的pid操作数据
         - A模块删除，B模块新增：生成同一个批次号的，1条删除数据和1新增数据
      2. 把通知节点改为待处理通知leader服务器，follow服务器返回前端受理成功
      3. leader服务器接收需处理通知，从操作记录表中获取操作数据，后续流程与请求至leader服务器流程一致
      7. leader服务器处理完成后更新通知节点数据为已处理
    ::: tip 服务端/客户端根据zk节点的更新/新增/删除的简易流程
    服务端找到需要在客户端服务器节点上新增pid节点，pid节点data状态值为ADDING -> 客户端服务器接收到新增节点，把新增节点数据获取后放进资源池后，将新增pi节点的状态值改为ADDED -> 服务端后续更新zk 映射的内存数据（PidCollection#pidMap）；更新/删除同理
    :::

### 客户端资源账户指令数同步资源服务器流程 - leader/客户端处理流程
1. 客户端服务器获取到本服务器所分配的pid
2. 根据pid#userName 从redis处获取每个pid对应的指令数使用情况
3. 资源系统leader和客户端服务器使用RLocalCachedMap存储一份与redis分布式缓存相关联的本地缓存
4. 客户端服务器每个pid在处理某个业务处理时，会记录处理前及处理后的pid使用量，更新本地缓存，通过redis机制更新至reids中；其它服务器如果包含该key本地缓存时，会先将本地缓存至为失效，重新从redis获取pid使用情况
5. 服务端启动一个守护线程，每隔一段时间从redis获取对应pid使用量

### 核心类介绍  
| 名称 | 含义 |
| :---: | :--- |
| LocalPidResouce | 数据库数据与内存中数据映射和交互的纽带|
| pidUtils | 承接运营人员页面操作pid后的处理，同时更新至zk |
| PidCollection | pid收集器，包含两个核心内存数据<br> 1. pidWithModule：数据库映射的内存数据 <br> 2. pidMap: zk节点映射的内存数据 |
| moduleNode | 操作模块及客户端服务器（InstanceVo）zk节点的核心类 |
| pidNode | 操作具体pid zk节点的核心类  |
| AbsNode | moduleNode&&pidNode父类，主要抽离了处理zk节点的公共方法  |

## 🔍 常见问题
### 问题1 
**问题描述：** 操作zk只允许leader服务器处理；运营人员操作前端页面时将请求到资源系统follow服务器时，该如何将动作通知给leader服务器操作zk，同时作用到相应的客户端服务器<br>
**解决方案：** 上面流程已详细描述；简单描述下：follow服务器将运营人员操作进行拆分成操作记录表 -> 通知leader服务器处理操作记录表 -> 先更新数据库映射的本地内存 -> 通知所关联的模块#系统#服务器的zk节点 data值为ING -> 客户端服务器将该pid进行处理，处理成功后修改zk节点值为完成状态 -> leader服务器更新zk映射的内存数据 -> 更新数据库数据

### 问题2  
**问题描述：** 客户端服务器down机，如何释放资源账户，待系统重启后该如何分配资源账户（线上服务器docker管理，发布的时候ip会变化，难以将down机前后服务器进行绑定）  
**解决方案：** 运营操作人员首先会将部门#模块#系统#服务器数量进行预配置，资源系统leader服务器启动时将部门#模块#系统所分配的资源账户列表按服务器数量进行内存分片，并以信号量控制分配的服务器数量；当客户端服务器启动时从leader服务器内存进行分配至客户端服务器；  
客户端down机：
1. 客户端服务器所对应的临时节点会发出remove事件
2. 资源系统leader服务器监听remove事件，删除zk映射的内存中客户端服务器对应的资源账户，更新数据库映射的内存中down的那台服务器的资源数据`<ModeuleEnum: [InstanceVo: List<PID>]>`;将对应的InstanceVo状态置于未分配，同时删除数据库中该Instance数据
3. 客户端服务器重启时会重新监听模块节点，并在模块节点下创建临时节点 -> leader服务器创建持久化节点 -> 从数据库映射的内存中拿到状态为未分配的`<InstanceVo: List<PID>>` -> 新增Instance数据，将id写入InstanceVo，把pid列表节点写入Instance的子节点 -> 客户端服务器将pid列表节点登录完成后，修改pid节点状态为完成时 -> leader服务器更新zk映射节点的内存数据

### 问题3  
**问题描述：** 是否能动态的对客户端服务器进行扩容和收缩
**解决方案：** 目前的设计是客户端服务器数量是需要与资源系统中模块#系统#服务器数量配置一致的；因为leader服务器就是通过该值进行内存预分配，但其实也是有办法的，如果某个客户端服务器数量发生变化，资源系统leader服务器能够感知，根据该客户端所分配的`List<pid>`进行重新分配；基于将影响降低最低的前提下，只需要从已分配的每台客户端服务器中抽出少量的`List<Pid>`;
1. 先将leader服务器数据库映射的内存数据#已分配的客户端服务器`List<Pid>`进行删除
2. 操作对应的zk数据节点，客户端响应删除成功后，删除leader服务器与zk映射的内存数据
3. 待所有删除的客户端服务器处理完成后，数据库映射的内存数据put分配给新增客户端服务器的`List<pid>；
4. 在新增服务器的持久化节点下创建pid zk节点，待客户端服务器处理成功后，更新leader服务器zk映射的内存数据
